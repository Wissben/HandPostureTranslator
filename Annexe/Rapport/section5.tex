\newpage
\section{Comparaison entre des approches}
\paragraph{}Nous commençons d'abord par donner les tableaux récapitulatifs suivants :
\paragraph{Remarques : }
\begin{itemize}
	\item Tous les essayes se sont faite en 4 ré-apprentissages 
	\item chaque couche de sortie dispose de la fonction d'activation \textbf{softmax}\footnote{Fonction mathématiques de normalisation \url{https://en.wikipedia.org/wiki/Softmax_function}}, en raison de la nature de la codification choisie.
	\item La liste des fonctions d'apprentissages utilisées au complet est cité dans \cite{KerasOpt}
\end{itemize}
\begin{table}[H]
	\centering
	\label{zeros}
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{|l|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{\multirow{2}{*}{Architecture}}               & \multicolumn{3}{c|}{Régression moyenne}         \\ \cline{2-4} 
		\multicolumn{1}{|c|}{}                                            & Apprentissage & Validation & Évaluation \\ \hline
		{[}40, 50, 50, 10{]} {[}'relu', 'relu', 'relu', 'relu'{]} : Nadam & 0.9719        & 0.9538     & 0.9574     \\ \hline
		{[}40, 50, 50, 20{]} {[}'relu', 'relu', 'relu', 'relu'{]} : Nadam & 0.9717        & 0.9614     & 0.9568     \\ \hline
		{[}40, 50, 50, 30{]} {[}'relu', 'relu', 'relu', 'relu'{]} : Nadam & 0.9729        & 0.95705    & 0.9559     \\ \hline
		{[}40, 50, 20{]} {[}'relu', 'relu', 'relu'{]} : Nadam             & 0.9703        & 0.9515     & 0.9550     \\ \hline
		{[}40, 50, 50, 20{]} {[}'relu', 'relu', 'relu', 'relu'{]} : Nadam & 0.9723        & 0.95113    & 0.9547     \\ \hline
	\end{tabular}
	}
	\caption{Meilleures architectures sur les données de teste pour l'approche naïve (\ref{naiveApproache}) avec partitionnement aléatoire(\ref{randomPartLearning})}
\end{table}
\paragraph{Commentaires :}
La première approche(voir \ref{naiveApproache}) avec partitionnement aléatoire a  donné d'assez bon résultats, il n'y a pas eu un sur-apprentissage apparent, les données de testes on été plutôt bien prédites, comme expliqué dans \ref{naiveApproache}, la discrimination a été effectué tant bien que mal, principalement dû au motif(pattern) des valeurs manquantes qui dépend grandement des geste de chaque utilisateur, cette approche reste a être testé avec des instances non-présentes dans le data-set.
\begin{table}[H]
	\centering
	\label{shit}
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{|l|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{\multirow{2}{*}{Architecture}}                   & \multicolumn{3}{c|}{Régression}         \\ \cline{2-4} 
		\multicolumn{1}{|c|}{}                                                & Apprentissage & Validation & Évaluation \\ \hline
		{[}40, 20{]} {[}'relu', 'relu'{]} : Adagrad  & 0.8948        & 0.8966     & 0.7918     \\ \hline
		{[}40, 20{]} {[}'relu', 'relu'{]} : Adagrad  & 0.8638        & 0.8639     & 0.7906     \\ \hline
		{[}40, 40{]} {[}'relu', 'relu'{]} : Adagrad  & 0.9057        & 0.8965     & 0.7797     \\ \hline
		{[}40, 10{]} {[}'relu', 'relu'{]} : Adadelta & 0.9396        & 0.9365     & 0.7744     \\ \hline
		{[}40, 30{]} {[}'relu', 'relu'{]} : Adagrad  & 0.8856        & 0.8827     & 0.7708     \\ \hline
	\end{tabular}
	}
	\caption{Meilleures architectures sur les données de teste pour l'approche avec naïve (\ref{naiveApproache}) avec partitionnement one-user-left(\ref{oneLeftLearning})}
\end{table}

\paragraph{Commentaires :	}
La même approche que précédemment (voir \ref{naiveApproache}) mais avec un partitionnement one-user-left, malheureusement cette approche n'a pas donnée d'aussi bon résultats que la précédente, principalement dû au fait que chaque utilisateur avait une manière différente de réaliser les gestes demandés ainsi qu'au non-étiquetage des marqueurs, les données manquantes sont donc très différentes selon l'utilisateur, ainsi les données du testeur sont très mal prédites, ceci est un cas typique de sur-apprentissage.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{|l|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{\multirow{2}{*}{Architecture}}      & \multicolumn{3}{c|}{Régression}         \\ \cline{2-4} 
		\multicolumn{1}{|c|}{}                                   & Apprentissage & Validation & Évaluation \\ \hline
		{[}40, 20{]} {[}'relu', 'relu'{]} : Adadelta             & 0.9988        & 0.9968     & 0.9632     \\ \hline
		{[}40, 40{]} {[}'relu', 'relu'{]} : SGD                  & 0.9990        & 0.9961     & 0.9620     \\ \hline
		{[}40, 50, 10{]} {[}'relu', 'relu', 'relu'{]} : Adadelta & 0.9986        & 0.9977     & 0.9602     \\ \hline
		{[}40, 30{]} {[}'relu', 'relu'{]} : RMSprop              & 0.9982        & 0.9967     & 0.9577     \\ \hline
		{[}40, 10{]} {[}'relu', 'relu'{]} : Adam                 & 0.9983        & 0.9963     & 0.9569     \\ \hline
	\end{tabular}
	}
	\caption{Meilleures architectures sur les données de teste pour l'approche avec Clustering (\ref{clusterApproache}) avec partitionnement one-user-left(\ref{oneLeftLearning})}
\end{table}
\paragraph{Commentaires :	}
Les remarques qui sautent aux yeux sont bien entendu le très bon score à l'évaluation (meilleur que dans \ref{zeros}) ainsi que le faible écart entre données d'apprentissage, de validation et de teste, malgré un partitionnement identique à celui de \ref{shit}, la différence faite par le clustering pour le ré-étiquetage des marqueurs et la gestion des valeurs manquantes a grandement aidé a donné un pattern similaires à ces valeurs pour chaque utilisateur, cette approche a donc couvert un peu plus l'espace de données possibles.